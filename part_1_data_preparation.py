# -*- coding: utf-8 -*-
"""part 1 - Data Preparation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fpXdE-3vXOLDxoyDyAG8UUGWrQker5Td

#### Importing Libraries and Loading the Dataset
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install pandas
import pandas as pd
!pip install numpy
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
import re
import numpy as np
import pandas as pd
import string
# %matplotlib inline

import kagglehub

# Download latest version
path = kagglehub.dataset_download("alperenmyung/social-media-advertisement-performance")

print("Path to dataset files:", path)

df1 = pd.read_csv(path + "/ad_events.csv")
df2 = pd.read_csv(path + "/ads.csv")
df3 = pd.read_csv(path + "/campaigns.csv")
df4 = pd.read_csv(path + "/users.csv")

print(df1.head())
print(df2.head())
print(df3.head())
print(df4.head())

df4.user_id.value_counts() #this shows that we have duplicated event_id but its suppuse to be uniqe

"""#### cleaning the duplicates in users data frame (df4):

###### 1. remove the single row where user_gender == 'Other' for each user who appears multiple times in the dataset and has exactly one occurrence of "Other" in their gender values
"""

df4_clean = df4.copy()


multi = df4_clean.groupby('user_id')['user_id'].transform('size') > 1


exact_one_other = (
    df4_clean.groupby('user_id')['user_gender']
       .transform(lambda x: (x == 'Other').sum() == 1)
)


df4_clean = df4_clean[~(multi & exact_one_other & (df4_clean['user_gender'] == 'Other'))]
df4_clean

"""###### 2. identify users (user_id) who have both "Female" and "Male" entries in their records, and removes only the "Female" rows for those usersâ€”keeping the "Male" entries when both genders exist."""

has_female = df4_clean.groupby('user_id')['user_gender'] \
    .transform(lambda x: 'Female' in x.tolist())

has_male = df4_clean.groupby('user_id')['user_gender'] \
    .transform(lambda x: 'Male' in x.tolist())

drop_mask = has_female & has_male & (df4_clean['user_gender'] == 'Female')

df4_clean = df4_clean[~drop_mask]
df4_clean

"""###### 3. remove the single row where country == 'United States' for each user who appears multiple times in the dataset and has exactly one occurrence of "United States"
"""

# find event_ids that appear more than once
multi = df4_clean.groupby('user_id')['user_id'].transform('size') > 1

# find event_ids where exactly one row has gender "other"
exact_one_other = (
    df4_clean.groupby('user_id')['country']
       .transform(lambda x: (x == 'United States').sum() == 1)
)

# drop only the 'other' row in those groups
df4_clean = df4_clean[~(multi & exact_one_other & (df4_clean['country'] == 'United States'))]
df4_clean

"""###### 4. manually removes a few specific rows from the dataset by their index values, as part of a random data reduction process"""

df4_clean = df4_clean.drop(index=[1294, 1739 ,5384,7706 , 8207 , 802	, 5009 , 1996 , 4896])
df4_clean

"""###### verify that no duplicate user_id values remain in the dataset by checking the number of occurrences of each user_id"""

df4_clean.user_id.value_counts()

"""#### Uniting Tables

###### Merge df1 and df2 on the common column ad_id using an outer join to include all rows from both tables.
"""

merged_df1_2= pd.merge(df1, df2, on=['ad_id'], how='outer')
merged_df1_2

"""###### Merge merged_df1_2 and df3 on the common column campaign_id using an inner join to keep only the rows with matching campaign information and remove unrelated rows without additional data"""

merged_df1_2_3= pd.merge(merged_df1_2, df3, on=['campaign_id'], how='inner')
merged_df1_2_3

"""###### Merge `merged_df1_2_3` and `df4_clean` on the common column `user_id` to combine user-related information with the existing merged dataset."""

merged_df1_2_3_4= pd.merge(merged_df1_2_3, df4_clean, on=['user_id'], how='inner')
merged_df1_2_3_4

"""##### Save the merged dataset as a CSV file named `merged_df1_2_3_4.csv` without including index numbers, and then load it back into a new DataFrame called `df`."""

merged_df1_2_3_4.to_csv('merged_df1_2_3_4.csv', index=False)
df = pd.read_csv('merged_df1_2_3_4.csv')

df

"""#### Converting date columns to datetime format and extracting individual time components such as year, month, day, hour, minute, and second from the timestamp column for further analysis.

"""

df["start_date"] = pd.to_datetime(df["start_date"])
df["end_date"] = pd.to_datetime(df["end_date"])

df["timestamp"] = pd.to_datetime(df["timestamp"])

df["year"]  = df["timestamp"].dt.year
df["month"] = df["timestamp"].dt.month
# Extracting both numeric and named month columns from the timestamp
df["month_name"] = df["timestamp"].dt.month_name()

# Ordering month names correctly for proper sorting and visualization
month_order = ["January","February","March","April","May","June","July",
               "August","September","October","November","December"]
df["month_name"] = pd.Categorical(df["month_name"], categories=month_order, ordered=True)

df["day"]   = df["timestamp"].dt.day
df["hour"]  = df["timestamp"].dt.hour
df["minute"] = df["timestamp"].dt.minute
df["second"] = df["timestamp"].dt.second

"""#### Adding a new column that identifies whether the day of the week falls between Saturday and Monday (inclusive).
#### Values are set to 1 for these days and 0 otherwise.
"""

df["is_weekend"] = df["day_of_week"].isin(["Saturday", "Sunday", "Monday"]).astype(int)

df

"""#### Save the prepared dataset as a CSV file named `df_after_prep.csv` without including index numbers."""

df.to_csv('df_after_prep.csv', index=False)

from google.colab import drive
drive.mount('/content/drive')
df.to_csv('/content/drive/MyDrive/df_after_prep.csv', index=False)